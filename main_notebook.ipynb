{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# University of Minnesota \n",
    "## Program for Clinical AI (P4AI) in the Center for Learning Health System Sciences\n",
    "\n",
    "## NLP Task - Analyst Position\n",
    "### Kate Weber\n",
    "`kgweber@umich.edu`\n",
    "\n",
    "# Task Approach\n",
    "\n",
    "I see this task as a question of detecting the presence or absence of semantic information from one data input in a longer data input. The biomedical writing is extremely complex, with arcane syntax and vocabulary. While preparing my approach, I read the [SemRep](https://link.springer.com/article/10.1186/s12859-020-3517-7) paper so see how the original algorithm worked so I didn't apply methods that amplified its weaknesses.\n",
    "\n",
    "I reviewed a number of other papers such as [Verma and Muralikrishna](https://ieeexplore.ieee.org/abstract/document/9198445) and [Chen et al.](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1044-0) for ideas on my approach but found that most of them were interested in whether Sentence A meant _the same thing_ as Sentence B, not whether A _was contained_ within B.\n",
    "\n",
    "The authors of SemRep performed semantic analysis of the content and explicitly linked it to sources of biomedical knowledge. I decided to apply a transformer model to the problem, choosing one that already had been exposed to large quantities of biomedical information. [BioClinicalBERT](https://arxiv.org/abs/1904.03323) has worked well for me on tasks involving parsing clinical notes, so I decided to bring it to the task.\n",
    "\n",
    "## Technical Approach\n",
    "\n",
    "* **Coding Platform** I chose Python for its familiarity and access to some great deep learning APIs. (If this were purely a statistical analysis, I might have opted for R, depending on the audience for the outcome)\n",
    "* **Deep Learning Stack** I opted for Keras because it produces very readable code and is quick to put together and re-organize. This algorithm needs optimization (a lot), so if we were talking about taking it to a more robust state, I'd rewrite in TensorFlow.\n",
    "* **Compute** Linux with a NVIDIA Tesla V100 GPU, CUDA 12.1. There are probably artifacts of bizarre NVIDIA/Tensorflow/CUDA/Linux-headers dependency errors in this notebook - I chose to work around them, but again: for production, this would need to be resolved.\n",
    "\n",
    "## Modeling Approach\n",
    "Since Bert is doing the bulk of the semantic work for me, the core questions were really how to present the information to a model for inference. I chose to trial two architectures:\n",
    "* A **single input** in which I concatenated the predication with the sentence and let Bert sort it out; and\n",
    "* **Two inputs** - one the predication and one the sentence - with an extra hidden layer for the longer sentence. I tried this on the theory that the sentence might require more transformation before it could be evaluated against the predication input.\n",
    "\n",
    "## Data Prep\n",
    "\n",
    "I did some very basic exploratory analysis before preparing the data in `data_prep.ipynb`. I cleaned redundant spaces from the `SENTENCE` variable and created a new column called `predication` in which I merged the subject, predicate, and object into a single mini-sentence.  I also created a new column called `concatenated` in which I concatenated the new `predication`, a `[SEP]` token, and the cleaned `sentence`.\n",
    "\n",
    "Each of these columns - `sentence`, `predication`, and `concatenated` - was tokenized by the BertTokenizer and stored in a dictionary. I padded the sentences and concatenated columns out to 250 tokens. Some of them are much longer, but I was running into memory issues, so cut it to a value that still contained the entirety of the overwhelming majority of sentences. The predications were padded to 32 tokens.\n",
    "\n",
    "I took a test split of 20% of the rows and created a train split of the remaining 80%. Validation during training took another 20% from the train split, leaving the test set solely for evaluation after training.  The dataset was stored in a `pickle` file for re-use so that all training and evaluation work was definitively running over the same data.\n",
    "\n",
    "## Training and Evaluation\n",
    "\n",
    "After early experiments to prove the concept, I used [Weights and Balances](https://api.wandb.ai/links/dlhs_rau/t5z7sl3u) to track experiments and automate my hyperparameter search.  The link will take you to a report showing the accuracy and validation-loss curves for selected runs. I very quickly learned that the two-input model (when I could get it to run) produced results no better than the concatenated model. It frequently ran out of memory and I abandoned it to avoid a nastygram from the cluster's manager. I manipulated batch size, learning rate, the number of epochs, and dropout size as I tuned. \n",
    "\n",
    "The models fell into three basic groups:\n",
    "* Models that overtrained instantly: without dropout, the models quickly memorized this dataset.\n",
    "* Models that never trained at all. Although some dropout was helpful, if it went too high, or learning rate was wrong, the model just sat on the 0.50 accuracy line and flipped a coin every time\n",
    "* Models that were _just right_: a very limited number. I learned to push my batch size up as high as the GPU would take (8, if the stars aligned just so) and hold dropout around .40 to slow the learning process to a manageable rate. The selected model (`resilient-music-93`) was generated with hyperparameters identified by run `helpful-cosmos-99` after the sweeps concluded.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18fed96f5686ae4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Code\n",
    "\n",
    "The code below is designed so it can be run once with a given configuration, or as part of a sweep. Lines that actually execute the sweep have been commented out to prevent an accident - as has the code that saves a model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc0c7eb15cf8bab"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-11T14:21:54.906683978Z",
     "start_time": "2023-11-11T14:21:47.612724141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 09:21:48.691545: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-11 09:21:48.691598: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-11 09:21:48.693717: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-11 09:21:48.876734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-11 09:21:52.110607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.147238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.147472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.149560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.149738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.149891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.220901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.221056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.221181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 09:21:52.221273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6068 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:4c:00.0, compute capability: 8.6\n",
      "2023-11-11 09:21:52.635526: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "from transformers import TFBertModel\n",
    "\n",
    "bmodel = TFBertModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We bring in the prepared data\n",
    "(see [data_prep.ipynb](data_prep.ipynb) for the prep tasks)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96cb4cc571be6f9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f2cc2a6b5df3ee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:21:54.918295409Z",
     "start_time": "2023-11-11T14:21:54.907192968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence dict_keys(['text', 'max_length', 'encoded', 'test', 'train'])\n",
      "predication dict_keys(['text', 'max_length', 'encoded', 'test', 'train'])\n",
      "concatenated dict_keys(['text', 'max_length', 'encoded', 'test', 'train'])\n",
      "predicate dict_keys(['text', 'test', 'train'])\n",
      "label dict_keys(['text', 'test', 'train'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    print(k, v.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Function\n",
    "\n",
    "This can be run once or take inputs from a hyperparameter sweep. This model architecture is for the single-input, concatenated model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e8c57974b532b90"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6797205438211edf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:21:54.954219418Z",
     "start_time": "2023-11-11T14:21:54.920999898Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # these are config values that came from wandb sweep passes\n",
    "    config_defaults = {\n",
    "        \"model_name\": \"concatenated_model\",\n",
    "        \"dropout\": 0.43,\n",
    "        \"learning_rate\": 0.000030,\n",
    "        \"epsilon\": 1e-08,\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"metric\": [\"accuracy\"],\n",
    "        \"epoch\": 6,\n",
    "        \"batch_size\": 8,\n",
    "        \"validation_split\": 0.2,\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"uminn\",\n",
    "        config=config_defaults\n",
    "    )\n",
    "\n",
    "    train_x = dataset['concatenated']['train']\n",
    "    train_y = dataset['label']['train']\n",
    "\n",
    "    # define the model\n",
    "\n",
    "    inputs = Input(shape=(250,), dtype='int32')\n",
    "    bert_output = bmodel(inputs)\n",
    "    bert_output = bert_output[1]\n",
    "    dropout = Dropout(wandb.config.dropout)(bert_output)\n",
    "    dense = Dense(1, activation='sigmoid')(dropout)\n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    opt = Adam(learning_rate=wandb.config.learning_rate,\n",
    "               epsilon=wandb.config.epsilon)\n",
    "\n",
    "    model.compile(loss=wandb.config.loss,\n",
    "                         optimizer=opt,\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_x,\n",
    "                     train_y,\n",
    "                     epochs=wandb.config.epoch,\n",
    "                     validation_split=wandb.config.validation_split,\n",
    "                     callbacks=[WandbMetricsLogger()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code I was using for the two-input model is in far less-talkative workbook in `./extras/two_inputs.ipynb`. Its heart is this architecture:\n",
    "\n",
    "\n",
    "```python\n",
    "train_x = (dataset['predication']['train'], dataset['sentence']['train'])\n",
    "train_y = dataset['label']['train']\n",
    "\n",
    "input_preds = Input(shape=(32,), dtype='int32')\n",
    "input_sents = Input(shape=(250,), dtype='int32')\n",
    "preds_bert_output = bmodel(input_preds)\n",
    "sents_bert_output = bmodel(input_sents)\n",
    "preds_bert_output = preds_bert_output[1]\n",
    "sents_bert_output = sents_bert_output[1]\n",
    "hidden_sents_1 = Dense(32, activation='relu')(sents_bert_output)\n",
    "dropout_sents_1 = Dropout(wandb.config.dropout)(hidden_sents_1)\n",
    "concat = Concatenate()([preds_bert_output, hidden_sents_1])\n",
    "dense = Dense(1, activation='sigmoid')(concat)\n",
    "two_input_model = Model(inputs=[input_preds, input_sents], outputs=dense)\n",
    "```\n",
    "\n",
    "### Training Curves\n",
    "These plots are excerpted from the report at [Weights and Balances](https://api.wandb.ai/links/dlhs_rau/t5z7sl3u):\n",
    "\n",
    "![Images of validation-loss and accuracy curves](extras/training-val-curves.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185a4dbc8db02500"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b144505f9c34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just do one with given hyperparameters\n",
    "\n",
    "model_file_name = 'concat_resilient_music.keras'\n",
    "# \n",
    "concat_model = train_model()\n",
    "# commented out for safety\n",
    "# concat_model.save(f'models/{model_file_name}')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sweep Configuration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b9350754f607c79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd34af1-98de-435e-a836-c34bd17ca870",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-11T14:07:32.490387552Z",
     "start_time": "2023-11-11T14:07:32.489423900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Or do a hyperparameter sweep to find some good settings\n",
    "\n",
    "# sweep_config = {\n",
    "#     'method': 'bayes',\n",
    "#     'name': 'umn-concatenation',\n",
    "#     'metric': {\n",
    "#         'name': 'epoch/val_loss',\n",
    "#         'goal': 'minimize'\n",
    "#     },\n",
    "#     'parameters': {\n",
    "#         \"dropout\": {'min': 0.20, 'max': 0.80},\n",
    "#         \"learning_rate\": {'values': [5e-5, 3e-5, 2e-5]},\n",
    "#         \"epoch\": {'min': 6, 'max': 10},\n",
    "#         \"batch_size\": {'values': [2, 4, 6, 8]}\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"uminn\")\n",
    "# wandb.agent(sweep_id, train_model, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0ab189b319531",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The shortest-version: On our test set, the model I chose has\n",
    "* **Precision**: 0.720446\n",
    "* **Recall**: 0.720000\n",
    "* **F1 Score**: 0.718635\n",
    "\n",
    "This was the closest I could get with this approach, and even so, these test-set results are much poorer than what we get from the training set. I am looking forward to talking to you to get a reality check on how this performance actually rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f2e2f36fde38f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T14:07:32.490789022Z"
    }
   },
   "outputs": [],
   "source": [
    "# load a model from disk\n",
    "concat_model = load_model('models/concat_resilient_music.keras', custom_objects={\"TFBertModel\": TFBertModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970d063e654fac8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T14:07:32.492252800Z"
    }
   },
   "outputs": [],
   "source": [
    "concat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e50dc8aa8fe46",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T14:07:32.534591378Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred = concat_model.predict(dataset['concatenated']['test'])\n",
    "train_pred = concat_model.predict(dataset['concatenated']['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subsetting the Data\n",
    "\n",
    "I was curious whether certain predicates performed better than others - whether some verbs are more difficult to generalize than others, and whether the imbalance between the classes led the algorithm to ignore one kind of predicate in favour of the others. So I ran the evaluation over each subset of the data.\n",
    "\n",
    "On the whole, it appears that performance is balanced across the classes of predicates. The order of performance between the classes is different between train and test, leading me to believe that the differences may just be noise.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5362d00eeb4110"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = {'test': pd.DataFrame({'y_pred': test_pred.round().flatten(), \n",
    "                                            'label': dataset['label']['test'],\n",
    "                                            'predicate': dataset['predicate']['test'],\n",
    "                                            }),\n",
    "               'train': pd.DataFrame({'y_pred': train_pred.round().flatten(), \n",
    "                                             'label': dataset['label']['train'],\n",
    "                                             'predicate': dataset['predicate']['train'],\n",
    "                                             })\n",
    "               }\n",
    "test_class_df = pd.DataFrame(classification_report(predictions['test']['label'], \n",
    "                                                   predictions['test']['y_pred'], \n",
    "                                                   output_dict=True))\n",
    "test_class_df['dataset'] = 'test'\n",
    "train_class_df = pd.DataFrame(classification_report(predictions['train']['label'], \n",
    "                                                   predictions['train']['y_pred'], \n",
    "                                                   output_dict=True))\n",
    "train_class_df['dataset'] = 'train'\n",
    "class_df = pd.concat([test_class_df, train_class_df], axis=0)\n",
    "class_df['stratum'] = 'full_dataset'\n",
    "class_df = class_df.reset_index().rename(columns={'index': 'metric'})\n",
    "\n",
    "# subset by predicate\n",
    "for ds in ['train', 'test']:\n",
    "    for stratum in ['INHIBITS', 'INTERACTS_WITH', 'STIMULATES']:\n",
    "        df = predictions[ds]\n",
    "        df = df.loc[df.predicate == stratum]\n",
    "        strat_df = pd.DataFrame(classification_report(df.label, df.y_pred, output_dict=True))\n",
    "        strat_df['stratum'] = stratum\n",
    "        strat_df['dataset'] = ds\n",
    "        strat_df = strat_df.reset_index().rename(columns={'index':'metric'})\n",
    "        class_df = pd.concat([class_df, strat_df], axis=0)\n",
    "\n",
    "\n",
    "class_df.loc[class_df['metric']!='support', ['metric', 'weighted avg', 'dataset', 'stratum']].pivot(index = ['dataset', 'stratum'], columns='metric', values='weighted avg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T14:07:32.534892317Z"
    }
   },
   "id": "43901ef8da278883"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df2dc05dbc2f22",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:07:32.539485023Z",
     "start_time": "2023-11-11T14:07:32.535139625Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "palette={'maize': 'goldenrod', 'blue': 'midnightblue'}\n",
    "class_df['hue'] = class_df.apply(lambda row: 'maize' if row.dataset=='test' else 'blue', axis=1)\n",
    "oaplot=sns.catplot(class_df.loc[class_df['metric'] != 'support'], \n",
    "            x='metric', y='weighted avg',\n",
    "            col='dataset', row='stratum', kind='bar',\n",
    "            hue='hue', palette=palette,\n",
    "            height = 3, aspect = 2.5, legend=False)\n",
    "oaplot.fig.suptitle(\"Model performance subsetted by predicate and the train/test split\", y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Limitations and Next Steps\n",
    "\n",
    "## Limitations\n",
    "In order to be sure I delivered a sound product on short notice, I confined myself to a single overall approach - that of using BioClinicalBERT to cope with the semantic content in this exercise. That decision brought hardware constraints to the types of architecture I was able to explore.  Additionally, by choosing Keras, I was able to quickly prototype and experiment, but at the cost of customizability. I would be very interested in exploring whether all the layers of the BERT model are needed in the network, or whether we could slim it down.\n",
    "\n",
    "The dataset itself, with only 3000 examples, is a constraint on how well a neural network is able to perform without quickly overtraining. I overcame this in one project with a semi-supervised learning approach that took advantage of sub-labels in annotated text: it might be possible to do a similar job by definitively labelling all the negative examples in the dataset if we were to extract each object-verb-subject triplet from the sentences.\n",
    "\n",
    "## Possible Future Work\n",
    "\n",
    "If I were to continue on with this work, I would look at a few alternatives:\n",
    "* I would look for a more compact transformer and look for ways to optimize it with a more customized model\n",
    "* Generally speaking, I would look more closely at SemRep's parsing methods and search for improvements; I would then turn attention to other ways to examine the structure and semantics of the sentence to see whether there is an opportunity to compare the labeled predication with more of the sentence's content\n",
    "* I would consider a semi-supervised approach of extracting as many triplets as possible from the sentence and then having a model learn from our known-negative cases. When that is complete, we could put it to work looking for positive cases (which can't be mapped from the dataset as it exists because we don't know which triplet in the predication actually matched a triplet in the sentence) \n",
    "* Finally, I very much want to attempt a completely different approach with graph algorithms, examining whether we can generalize the predicate verbs well enough to allow shallow parsing to generate a document tree and then ask the algorithm to determine whether the predication in hand is a subgraph of the larger document's tree. I think I would explore whether it would be fruitful to parse a full `spaCy` document tree or do shallow parsing to obtain this graph. This paper by [Le, Can, and Collier](https://jbiomedsem.biomedcentral.com/articles/10.1186/s13326-022-00267-3) has an approach of walking a dependency tree to search for paths between nouns that I would like to explore."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c8be681cb83ff09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T14:07:32.535368308Z"
    }
   },
   "id": "4ca8785f79527076"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
